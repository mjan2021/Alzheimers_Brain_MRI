{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096a811-92e5-4290-933d-d13c780a77c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from models import *\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae0cb6-7144-4fb8-bb30-f7764ea85376",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yacs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e97999-d6ea-43b7-a1b6-3038f2504542",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yacs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myacs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CfgNode \u001b[38;5;28;01mas\u001b[39;00m CN\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yacs'"
     ]
    }
   ],
   "source": [
    "from yacs.config import CfgNode as CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09fc517-27f1-4413-933f-f383c2830ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=0., std=1.)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774320cb-75af-4a45-9f2c-4b07effb8dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('../car.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46331a6-91d3-45b6-b4e0-d3c42b5af15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet Model\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# model.load_state_dict(torch.load('../models/ResNet_adamx.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561218a-338e-4cfc-bd19-56cdbe9a7ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we will save the conv layer weights in this list\n",
    "model_weights =[]\n",
    "#we will save the 49 conv layers in this list\n",
    "conv_layers = []\n",
    "# get all the model children as list\n",
    "model_children = list(model.children())\n",
    "#counter to keep count of the conv layers\n",
    "counter = 0\n",
    "#append all the conv layers and their respective wights to the list\n",
    "for i in range(len(model_children)):\n",
    "    if type(model_children[i]) == nn.Conv2d:\n",
    "        counter+=1\n",
    "        model_weights.append(model_children[i].weight)\n",
    "        conv_layers.append(model_children[i])\n",
    "    elif type(model_children[i]) == nn.Sequential:\n",
    "        for j in range(len(model_children[i])):\n",
    "            for child in model_children[i][j].children():\n",
    "                if type(child) == nn.Conv2d:\n",
    "                    counter+=1\n",
    "                    model_weights.append(child.weight)\n",
    "                    conv_layers.append(child)\n",
    "print(f\"Total convolution layers: {counter}\")\n",
    "print(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aefb66-cd89-44d4-ad88-7355d6cd89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e0e4a-e14a-4de4-921f-32285535743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = transform(img)\n",
    "image = image.unsqueeze(0)\n",
    "image = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9674289c-c256-4c44-a004-0c14edba5acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = []\n",
    "names = []\n",
    "for layer in conv_layers[0:]:\n",
    "    image = layer(image)\n",
    "    outputs.append(image)\n",
    "    names.append(str(layer))\n",
    "print(len(outputs))\n",
    "#print feature_maps\n",
    "for feature_map in outputs:\n",
    "    print(feature_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e303d-7dd4-406e-89e0-13365f473001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed = []\n",
    "for feature_map in outputs:\n",
    "    feature_map = feature_map.squeeze(0)\n",
    "    gray_scale = torch.sum(feature_map,0)\n",
    "    gray_scale = gray_scale / feature_map.shape[0]\n",
    "    processed.append(gray_scale.data.cpu().numpy())\n",
    "for fm in processed:\n",
    "    print(fm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe22c55-8f60-4a33-8458-c85c4e4b461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16daba74-6c8e-4306-89b5-9f3c99f37ffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 50))\n",
    "for i in range(len(processed)):\n",
    "    a = fig.add_subplot(10, 5, i+1)\n",
    "    imgplot = plt.imshow(processed[i])\n",
    "    a.axis(\"off\")\n",
    "    a.set_title(names[i].split('(')[0], fontsize=30)\n",
    "plt.savefig(str('feature_maps.jpg'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca36b431-c5f7-4596-a769-004567fcd904",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed[5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974440bc-ca20-44ca-8436-3f9ba6dba034",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[10].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296fbe70-52d4-4479-bcee-ed84cda12a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\" Self attention Layer\"\"\"\n",
    "    def __init__(self,in_dim):\n",
    "        super(SelfAttention,self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        # self.activation = activation\n",
    "        \n",
    "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
    "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
    "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax  = nn.Softmax(dim=-1) #\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature \n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        m_batchsize,C,width ,height = x.size()\n",
    "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
    "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
    "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
    "        attention = self.softmax(energy) # BX (N) X (N) \n",
    "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
    "\n",
    "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
    "        out = out.view(m_batchsize,C,width,height)\n",
    "        \n",
    "        out = self.gamma*out + x\n",
    "        return out,attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade890bd-de9f-4b83-b9c4-9a6fa456caaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = SelfAttention(in_dim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc86a4d-78ad-4d76-8c9b-5bd43dd7699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[15].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b71da4-5e19-40b1-b9b4-6df44c805c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "out,attention = attn.forward(outputs[15].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cadbfb-8547-44ad-b6c4-137b39f9e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969b9a9-e3ab-4a3b-a0bb-470a81d27b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(attention_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb6d93-d2e8-4d9b-885d-b9d2537941a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(attention.squeeze().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69556ff3-de8a-40f7-a7ee-fbd08ab1604d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2e837f-c530-46bd-b9bf-fdc55a6ddb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "1, 2048, 14, 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b9bad8-8786-4dd8-9087-2f9be2038c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from timm.models.layers.attention_pool2d import AttentionPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300a6c0-daf0-4e09-97e8-7992dba52d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_timm = AttentionPool2d(in_features = feat_size=[2048, (14, 14)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbff883-c69a-48a9-8d12-df26079f8489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da7a2cb-0e84-4662-bc13-69ed041c1ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = random.sample(range(0, 30), 5)\n",
    "p = random.sample(range(0, 10), 5)\n",
    "r = random.sample(range(0, 10), 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21787a6-6dff-46b5-8232-a4ee4ae31649",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1, p, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4220340e-6471-43a1-9b75-eb1256c6efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'f1':f1,\n",
    "                   'p':p,\n",
    "                   'r':r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30792d31-874c-4da4-8df9-50eee96a15eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b6b6ed-1967-46e7-a08c-cb1352a8590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a6327-f381-4948-b6a4-f645a78b09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = dff.append(r_di, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585655f3-e8a6-427d-b637-4220d8ac4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b6416d-68a9-4ef6-a4a6-9a9dfb9b3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_di = {'f1':f1,\n",
    "                   'p':p,\n",
    "                   'r':r}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d607080b-a202-4e02-af11-be1aeb6fcdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b240bd-9fe9-4ec0-a956-1001a56296f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
